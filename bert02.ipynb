{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8248051,"sourceType":"datasetVersion","datasetId":4893646}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport re\nimport time\nimport datetime\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, matthews_corrcoef\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:38:17.058270Z","iopub.execute_input":"2024-04-27T20:38:17.059129Z","iopub.status.idle":"2024-04-27T20:38:33.928322Z","shell.execute_reply.started":"2024-04-27T20:38:17.059093Z","shell.execute_reply":"2024-04-27T20:38:33.927574Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-27 20:38:23.422412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-27 20:38:23.422524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-27 20:38:23.529644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the seed value for reproducibility\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Load data\ndf = pd.read_csv('/kaggle/input/dataset01/labeled_data.csv')\n\n# Function to preprocess tweets\ndef strip_all_entities(x):\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())\n\n# Preprocess tweets\ndf['tweet'] = df['tweet'].apply(strip_all_entities)\n\n# Train-test-validation split\ntrain, Teal = train_test_split(df, random_state=1508, shuffle=True, test_size=0.2)\ntest, validation = train_test_split(Teal, random_state=1508, shuffle=True, test_size=0.5)\n\n# Get sentences and labels\ntrn_sentences = train['tweet'].values\ntrain_labels = train['class'].values\ntst_sentences = test['tweet'].values\ntest_labels = test['class'].values\nval_sentences = validation['tweet'].values\nvalidation_labels = validation['class'].values\n\n# Tokenization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:38:58.424280Z","iopub.execute_input":"2024-04-27T20:38:58.425483Z","iopub.status.idle":"2024-04-27T20:39:00.296442Z","shell.execute_reply.started":"2024-04-27T20:38:58.425447Z","shell.execute_reply":"2024-04-27T20:39:00.295654Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a43a3509c94e85b72f4a9859ae8eba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4b77d60929408fb0f84f00ba2020fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8b8ce8b67d48b79983cfb171930845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37548099b1664277b291e3348da42513"}},"metadata":{}}]},{"cell_type":"code","source":"# Define function for tokenization and padding\ndef bert_encode(data, max_len):\n    input_ids = []\n    attention_masks = []\n\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(data[i],\n                                        add_special_tokens=True,\n                                        max_length=max_len,\n                                        padding='max_length',\n                                        truncation=True,\n                                        return_attention_mask=True)\n\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n\n    return np.array(input_ids), np.array(attention_masks)\n\nMAX_LEN = 64\ntrain_inputs, train_masks = bert_encode(trn_sentences, MAX_LEN)\nvalidation_inputs, validation_masks = bert_encode(val_sentences, MAX_LEN)\ntest_inputs, test_masks = bert_encode(tst_sentences, MAX_LEN)\n\n# Convert data to PyTorch tensors\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntest_inputs = torch.tensor(test_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntest_labels = torch.tensor(test_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\ntest_masks = torch.tensor(test_masks)\n\n# Create DataLoaders\nbatch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:39:27.146864Z","iopub.execute_input":"2024-04-27T20:39:27.147822Z","iopub.status.idle":"2024-04-27T20:39:42.011722Z","shell.execute_reply.started":"2024-04-27T20:39:27.147778Z","shell.execute_reply":"2024-04-27T20:39:42.010769Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 2e-5\nepochs = 20\n\n# Load BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n                                                      num_labels=3, \n                                                      output_attentions=False, \n                                                      output_hidden_states=False)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:41:02.077627Z","iopub.execute_input":"2024-04-27T20:41:02.078057Z","iopub.status.idle":"2024-04-27T20:41:05.542481Z","shell.execute_reply.started":"2024-04-27T20:41:02.078025Z","shell.execute_reply":"2024-04-27T20:41:05.541607Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d09c075a1b49b3931bdb98ad8314ae"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Check if GPU is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load BERT model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\nmodel.to(device)\n\n# Move tensors to the appropriate device\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_labels = batch[2].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:44:15.109655Z","iopub.execute_input":"2024-04-27T20:44:15.110036Z","iopub.status.idle":"2024-04-27T20:44:15.630947Z","shell.execute_reply.started":"2024-04-27T20:44:15.110005Z","shell.execute_reply":"2024-04-27T20:44:15.630082Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:45:11.399695Z","iopub.execute_input":"2024-04-27T20:45:11.400491Z","iopub.status.idle":"2024-04-27T20:45:11.406094Z","shell.execute_reply.started":"2024-04-27T20:45:11.400457Z","shell.execute_reply":"2024-04-27T20:45:11.404914Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define weight decay parameter\nweight_decay = 0.01  # Adjust the value as needed\n\n# Define optimizer with weight decay\noptimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=weight_decay)\n\n# Define learning rate scheduler\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\n\n# Training loop\nloss_values = []\n\nfor epoch_i in range(epochs):\n    print(\"\")\n    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n    print('Training...')\n    t0 = time.time()\n    total_loss = 0\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n        if step % 40 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)            \n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(f'  Average training loss: {avg_train_loss:.2f}')\n    print(f'  Training epoch took: {format_time(time.time() - t0)}')\n\n# Evaluate on validation set\nprint(\"\")\nprint(\"Running Validation...\")\n\nt0 = time.time()\nmodel.eval()\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\nfor batch in validation_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\nprint(f\"  Accuracy: {eval_accuracy/nb_eval_steps:.2f}\")\nprint(f\"  Validation took: {format_time(time.time() - t0)}\")\n\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-27T20:45:14.779414Z","iopub.execute_input":"2024-04-27T20:45:14.780418Z","iopub.status.idle":"2024-04-27T21:58:09.341309Z","shell.execute_reply.started":"2024-04-27T20:45:14.780380Z","shell.execute_reply":"2024-04-27T21:58:09.339806Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:13.\n  Batch    80  of    620.    Elapsed: 0:00:27.\n  Batch   120  of    620.    Elapsed: 0:00:41.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:10.\n  Batch   240  of    620.    Elapsed: 0:01:24.\n  Batch   280  of    620.    Elapsed: 0:01:38.\n  Batch   320  of    620.    Elapsed: 0:01:52.\n  Batch   360  of    620.    Elapsed: 0:02:06.\n  Batch   400  of    620.    Elapsed: 0:02:20.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:17.\n  Batch   600  of    620.    Elapsed: 0:03:31.\n\n  Average training loss: 0.28\n  Training epoch took: 0:03:38\n\n======== Epoch 2 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.21\n  Training epoch took: 0:03:39\n\n======== Epoch 3 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:10.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:17.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.17\n  Training epoch took: 0:03:38\n\n======== Epoch 4 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.12\n  Training epoch took: 0:03:39\n\n======== Epoch 5 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.08\n  Training epoch took: 0:03:39\n\n======== Epoch 6 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.06\n  Training epoch took: 0:03:39\n\n======== Epoch 7 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.03\n  Training epoch took: 0:03:39\n\n======== Epoch 8 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.03\n  Training epoch took: 0:03:39\n\n======== Epoch 9 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.02\n  Training epoch took: 0:03:39\n\n======== Epoch 10 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 11 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 12 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 13 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 14 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 15 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:36.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 16 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:36.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.01\n  Training epoch took: 0:03:39\n\n======== Epoch 17 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.00\n  Training epoch took: 0:03:39\n\n======== Epoch 18 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:57.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.00\n  Training epoch took: 0:03:39\n\n======== Epoch 19 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:11.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:50.\n  Batch   520  of    620.    Elapsed: 0:03:04.\n  Batch   560  of    620.    Elapsed: 0:03:18.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.00\n  Training epoch took: 0:03:39\n\n======== Epoch 20 / 20 ========\nTraining...\n  Batch    40  of    620.    Elapsed: 0:00:14.\n  Batch    80  of    620.    Elapsed: 0:00:28.\n  Batch   120  of    620.    Elapsed: 0:00:42.\n  Batch   160  of    620.    Elapsed: 0:00:56.\n  Batch   200  of    620.    Elapsed: 0:01:10.\n  Batch   240  of    620.    Elapsed: 0:01:25.\n  Batch   280  of    620.    Elapsed: 0:01:39.\n  Batch   320  of    620.    Elapsed: 0:01:53.\n  Batch   360  of    620.    Elapsed: 0:02:07.\n  Batch   400  of    620.    Elapsed: 0:02:21.\n  Batch   440  of    620.    Elapsed: 0:02:35.\n  Batch   480  of    620.    Elapsed: 0:02:49.\n  Batch   520  of    620.    Elapsed: 0:03:03.\n  Batch   560  of    620.    Elapsed: 0:03:17.\n  Batch   600  of    620.    Elapsed: 0:03:32.\n\n  Average training loss: 0.00\n  Training epoch took: 0:03:38\n\nRunning Validation...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     70\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m b_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 71\u001b[0m tmp_eval_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mflat_accuracy\u001b[49m(logits, label_ids)\n\u001b[1;32m     72\u001b[0m eval_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tmp_eval_accuracy\n\u001b[1;32m     73\u001b[0m nb_eval_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'flat_accuracy' is not defined"],"ename":"NameError","evalue":"name 'flat_accuracy' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Training loop\nfor epoch_i in range(epochs):\n    # Training code...\n    # Define flat_accuracy function\n    def flat_accuracy(preds, labels):\n        pred_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\n# After the training loop, you can use the flat_accuracy function\n# Example usage:\n# Calculate accuracy on validation set\nfor batch in validation_dataloader:\n    # Validation code...\n    logits = outputs[0].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:02:21.228988Z","iopub.execute_input":"2024-04-27T22:02:21.229364Z","iopub.status.idle":"2024-04-27T22:02:21.282781Z","shell.execute_reply.started":"2024-04-27T22:02:21.229335Z","shell.execute_reply":"2024-04-27T22:02:21.281870Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Predict on test set\nprint('Predicting labels for test sentences...')\n\nprediction_inputs = torch.tensor(test_inputs)\nprediction_masks = torch.tensor(test_masks)\nprediction_labels = torch.tensor(test_labels)\n\nbatch_size = 32\nprediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n\nmodel.eval()\npredictions , true_labels = [], []\n\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    predictions.append(logits)\n    true_labels.append(label_ids)\n\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:02:24.315329Z","iopub.execute_input":"2024-04-27T22:02:24.316174Z","iopub.status.idle":"2024-04-27T22:02:33.771953Z","shell.execute_reply.started":"2024-04-27T22:02:24.316132Z","shell.execute_reply":"2024-04-27T22:02:33.771014Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Predicting labels for test sentences...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/3707267552.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  prediction_inputs = torch.tensor(test_inputs)\n/tmp/ipykernel_34/3707267552.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  prediction_masks = torch.tensor(test_masks)\n/tmp/ipykernel_34/3707267552.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  prediction_labels = torch.tensor(test_labels)\n","output_type":"stream"},{"name":"stdout","text":"Done\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate MCC and other metrics\nmatthews_set = []\n\nfor i in range(len(true_labels)):\n    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n    matthews_set.append(matthews)\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nflat_true_labels = [item for sublist in true_labels for item in sublist]\nmcc = matthews_corrcoef(flat_true_labels, flat_predictions)\nprint(f'MCC: {mcc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:02:39.947838Z","iopub.execute_input":"2024-04-27T22:02:39.948203Z","iopub.status.idle":"2024-04-27T22:02:40.066344Z","shell.execute_reply.started":"2024-04-27T22:02:39.948172Z","shell.execute_reply":"2024-04-27T22:02:40.065310Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"MCC: 0.744\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(flat_true_labels, flat_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-04-27T22:02:43.903078Z","iopub.execute_input":"2024-04-27T22:02:43.903837Z","iopub.status.idle":"2024-04-27T22:02:43.921986Z","shell.execute_reply.started":"2024-04-27T22:02:43.903807Z","shell.execute_reply":"2024-04-27T22:02:43.920969Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.47      0.40      0.43       155\n           1       0.94      0.95      0.94      1893\n           2       0.90      0.89      0.89       430\n\n    accuracy                           0.90      2478\n   macro avg       0.77      0.75      0.76      2478\nweighted avg       0.90      0.90      0.90      2478\n\n","output_type":"stream"}]}]}